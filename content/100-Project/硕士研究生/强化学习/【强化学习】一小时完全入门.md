---
aliases: 
category: 视频
state: 重要但不紧急
tags:
  - 机器学习
  - 强化学习
  - 科研
from:
  - bilibili
url:
  - https://www.bilibili.com/video/BV13a4y1J7bw
stars: 
created: "2024-05-24 17:47"
updated: "2024-05-28 12:28"
dg-publish: 
media: https://www.bilibili.com/video/BV13a4y1J7bw
---
## 结构与元素
- [00:28](https://www.bilibili.com/video/BV13a4y1J7bw?t=28.853811#t=28.85) 什么是强化学习？
	- 在与环境的互动当中，为了达成一个目标而进行的学习过程
- [00:39](https://www.bilibili.com/video/BV13a4y1J7bw?t=39.713406#t=39.71) 第一层结构：基本元素
	- Agent：主体（特工、玩家）
	- Environment：环境
	- Goal：目标
- [02:09](https://www.bilibili.com/video/BV13a4y1J7bw?t=129.949262#t=02:09.95) 第二层结构：主要元素（Main Element）
	- State：所有决策所需的相关信息
	- Action：玩家作出的行动，会改变游戏状态
	- Reward：由最终目标所决定
	- 强化学习的主要过程就是State-Action-State-Action......，在每种状态下作出对应行动，行动又会改变当前状态。Reward则是行动造成的即时反馈，用于评价玩家行动的好坏。
	- Goal与Reward的区别：Goal是长远的目标，而Reward则是即时的反馈，如何通过设定合理的Reward来使得最终的结果靠近Goal是个重要命题。
- [05:28](https://www.bilibili.com/video/BV13a4y1J7bw?t=328.769106#t=05:28.77) 第三层结构：核心元素（Core Element）
	- Policy：策略，即在某个状态下应采取何种行动，类似于数学中的函数映射
		- 强化学习的目的就是**获得一个好的策略**
	- Value：价值，将来能够得到的所有奖励的期望值
		- State Value：输入状态，输出实数，即该状态的价值，代表预期将来会得到的所有奖励之和
			- 玩家应尽可能**进入价值最大的状态**
		- State-Action Value：在特定状态下，采取某种行动具有的价值
			- [ ] State-Action Value是如何计算的？怎么确定行动带来的价值？
		- 主流的强化学习算法都是依赖于价值方法的
- ![[Pasted image 20240524135129.png]]

## 实例-围棋
- [01:08](https://www.bilibili.com/video/BV13a4y1J7bw?t=68.383689#t=01:08.38) 以围棋举例：
	- 第一层结构：
		- Agent:玩家
		- Environment：棋盘、对手
		- Goal：赢得这局棋
	- [01:40](https://www.bilibili.com/video/BV13a4y1J7bw?t=100.65916#t=01:40.66) 第二层结构：
		- State1：空棋盘
		- Action1：星位落子
		- Reward1：0（除非赢棋，否则都是0）
		- 继续State2、Action2、Reward2......一直继续
	- [02:54](https://www.bilibili.com/video/BV13a4y1J7bw?t=174.165485#t=02:54.17) 第三层结构：
		- Policy：面对一个空棋盘，玩家应该在哪里落子？
		- Value：在某一状态下，每一个行动都对应一个价值，即状态行动价值。例如，面对一个空棋盘我有361种选择，每一种选择都对应一个实数值。
			- [03:49](https://www.bilibili.com/video/BV13a4y1J7bw?t=229.558334#t=03:49.56) 价值就是赢棋的概率，而选择价值最大的策略，会使得赢棋的概率最大
- [04:29](https://www.bilibili.com/video/BV13a4y1J7bw?t=269.859103#t=04:29.86) 强化学习的两个特点
	- Trial and Error：试错，在不断的尝试中学习
		- **监督学习**：棋谱书告诉我们在什么情况下应该怎样落子
		- **强化学习**：在不断的试错中寻找哪种落子方式更可能赢
	- [05:23](https://www.bilibili.com/video/BV13a4y1J7bw?t=323.225507#t=05:23.23) Delayed Reward：延迟奖励
		- 行动没有对应即时的奖励。**一个行动可能没有奖励，但一定有价值，因为价值是将来能够得到的奖励的期望值**
		- **如何计算每一步行动的价值（即每一步行动对于最终获得奖励的贡献）** 是一个关键问题。
- [06:48](https://www.bilibili.com/video/BV13a4y1J7bw?t=408.975855#t=06:48.98) 强化学习的核心问题：Exploration vs. Exploitation
	- [06:57](https://www.bilibili.com/video/BV13a4y1J7bw?t=417.071574#t=06:57.07) 利用：利用学习的价值函数去作出决策
		- 价值函数不一定是最优的，即对某些行动计算出了低价值，但其实它的真实价值很高
	- [07:09](https://www.bilibili.com/video/BV13a4y1J7bw?t=429.082366#t=07:09.08) 探索：尝试不同的行动以优化价值函数
	- 权衡利用与探索，是强化学习中的核心问题

## 强化学习算法-多臂老虎机

- [00:18](https://www.bilibili.com/video/BV13a4y1J7bw?t=18.311938#t=18.31) 什么是多臂老虎机
- [01:07](https://www.bilibili.com/video/BV13a4y1J7bw?t=67.797118#t=01:07.80) 特点分析
	- 只有一个状态
	- 奖励是即时的，不会有延迟奖励，当前的行动对之后也没有影响
- [02:00](https://www.bilibili.com/video/BV13a4y1J7bw?t=120.347212#t=02:00.35) 奖励的机制
	- 两个老虎机可能分别服从不同的概率分布，但这个分布是不变的
	- 没有延迟奖励的情况下，可以将行动价值定义为当前行动所获得的奖励
- [03:23](https://www.bilibili.com/video/BV13a4y1J7bw?t=203.866334#t=03:23.87) 价值函数与策略函数
	- [03:54](https://www.bilibili.com/video/BV13a4y1J7bw?t=234.58972#t=03:54.59) 价值函数：用平均值来估计行动价值
	- ![[Pasted image 20240524143834.png]]
	- 样本平均，最简单的估计价值方法
	- [04:36](https://www.bilibili.com/video/BV13a4y1J7bw?t=276.820365#t=04:36.82) 策略函数：选择价值最大的行动
		- ![[Pasted image 20240524144052.png]]
		- [05:11](https://www.bilibili.com/video/BV13a4y1J7bw?t=311.022449#t=05:11.02) 贪婪策略的问题
			- 如果价值一开始都是0，那么会一直选择一开始选择的那个
			- 如果强行要求所有行动都尝试一遍，那么会一直选择第一次尝试一遍之后价值最大的行动，但由于随机性的存在，第一次价值最大不一定是真的价值最大
			- 如果让价值函数的初始值特别大，那么被选择的行动，价值反而会变小，从而鼓励算法执行其他行动
				- 细节：初始值要计入平均计算，否则还是相当于把所有行动都尝试一遍，因为尝试一遍之后之前的初始值就被覆盖了，那还是谁价值最大选谁
		- [07:11](https://www.bilibili.com/video/BV13a4y1J7bw?t=431.906019#t=07:11.91) ε-Greedy
			- 大多数情况下作出贪婪选择，但以一定概率ε作出随机选择
- [07:48](https://www.bilibili.com/video/BV13a4y1J7bw?t=468.114644#t=07:48.11) 算法效果展示
- [09:03](https://www.bilibili.com/video/BV13a4y1J7bw?t=543.544262#t=09:03.54) 策略与价值
	- 策略：采用ε-Greedy策略作出选择
	- 价值：采用样本平均的方式来计算价值